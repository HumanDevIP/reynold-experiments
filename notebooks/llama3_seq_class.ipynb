{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "login() #Hugginface login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/transformed/cleaned_train_data.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/transformed/cleaned_test_data.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an instruction(prompt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = (\n",
    "    \"You are a skillful patent examiner with over 20 years of experience. \"\n",
    "    \"You are reviewing patent prior art. Examine if the following chunk of a patent description constitutes prior art for the given claim.\\n\"\n",
    "    \"DESCRIPTION: {text_b}\\n\"\n",
    "    \"CLAIM: {text}\"\n",
    ")\n",
    "\n",
    "# Create the prompt column in both datasets\n",
    "train_df['prompt'] = train_df.apply(lambda row: prompt_template.format(text_b=row['text_b'], text=row['text']), axis=1)\n",
    "test_df['prompt'] = test_df.apply(lambda row: prompt_template.format(text_b=row['text_b'], text=row['text']), axis=1)\n",
    "\n",
    "train_df.to_csv('../data/transformed/train_with_prompt.csv', index=False)\n",
    "test_df.to_csv('../data/transformed/test_with_prompt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Inspect the first row of the new DataFrame to verify the prompt column\n",
    "train_row = train_df.iloc[0]\n",
    "test_row = test_df.iloc[0]\n",
    "\n",
    "print(\"First row of the training data with prompt:\")\n",
    "print(train_row)\n",
    "\n",
    "print(\"First row of the test data with prompt:\")\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the train set to have a small validation set during training, since we have little data I will split only 10 %. I will also make a stratified split to have the same distribution of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_df, val_split_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "# Check the distribution of the target variable in both splits\n",
    "print(\"\\nDistribution of target variable in training split:\")\n",
    "print(train_split_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nDistribution of target variable in validation split:\")\n",
    "print(val_split_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to convert the data to a Hugginface Dataset and store it into my account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Huggingface Datasets\n",
    "train_dataset = Dataset.from_pandas(train_split_df)\n",
    "val_dataset = Dataset.from_pandas(val_split_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Merge the datasets into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset\n",
    "dataset_dict.push_to_hub(\"patentmatch_exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
